{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          2048        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          131328      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           16448       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           2080        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            33          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 7)            231         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 7)            0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 154,248\n",
      "Trainable params: 154,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/layers/core/lambda_layer.py:303: UserWarning: dqn is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(config, custom_objects,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "new_model = tf.keras.models.load_model('model/ddqn.keras', custom_objects={'K': K})\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/layers/core/lambda_layer.py:303: UserWarning: dqn is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(config, custom_objects,\n",
      "2024-11-11 12:37:04.121659: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-11 12:37:04.121763: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-11-11 12:37:04.318798: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-11-11 12:37:04.361780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward at 1000 is 1.374\n",
      "Reward at 2000 is 1.387\n",
      "Reward at 3000 is 1.408\n",
      "Reward at 4000 is 1.408\n",
      "Reward at 5000 is 1.414\n",
      "Reward at 6000 is 1.4155\n",
      "Reward at 7000 is 1.4175714285714285\n",
      "Reward at 8000 is 1.4135\n",
      "Reward at 9000 is 1.410111111111111\n",
      "Reward at 10000 is 1.4159\n",
      "Reward at 11000 is 1.4172727272727272\n",
      "Reward at 12000 is 1.42\n",
      "Reward at 13000 is 1.422076923076923\n",
      "Reward at 14000 is 1.424\n",
      "Reward at 15000 is 1.4262\n",
      "Reward at 16000 is 1.4275\n",
      "Reward at 17000 is 1.4284117647058823\n",
      "Reward at 18000 is 1.4313888888888888\n",
      "Reward at 19000 is 1.4312631578947368\n",
      "Reward at 20000 is 1.431\n",
      "Reward at 21000 is 1.432\n",
      "Reward at 22000 is 1.4294090909090909\n",
      "Reward at 23000 is 1.4311739130434782\n",
      "Reward at 24000 is 1.4315833333333334\n",
      "Reward at 25000 is 1.43284\n",
      "Reward at 26000 is 1.4317692307692307\n",
      "Reward at 27000 is 1.4292222222222222\n",
      "Reward at 28000 is 1.4288571428571428\n",
      "Reward at 29000 is 1.4295862068965517\n",
      "Reward at 30000 is 1.4302\n",
      "Reward at 31000 is 1.430516129032258\n",
      "Reward at 32000 is 1.42965625\n",
      "Reward at 33000 is 1.4297575757575758\n",
      "Reward at 34000 is 1.43\n",
      "Reward at 35000 is 1.4293142857142858\n",
      "Reward at 36000 is 1.4287777777777777\n",
      "Reward at 37000 is 1.4275945945945947\n",
      "Reward at 38000 is 1.428157894736842\n",
      "Reward at 39000 is 1.4286410256410256\n",
      "Reward at 40000 is 1.4291\n",
      "Reward at 41000 is 1.429390243902439\n",
      "Reward at 42000 is 1.4300238095238096\n",
      "Reward at 43000 is 1.4287674418604652\n",
      "Reward at 44000 is 1.4284772727272728\n",
      "Reward at 45000 is 1.428688888888889\n",
      "Reward at 46000 is 1.4288478260869566\n",
      "Reward at 47000 is 1.4290212765957446\n",
      "Reward at 48000 is 1.4299583333333334\n",
      "Reward at 49000 is 1.4298775510204083\n",
      "Reward at 50000 is 1.42998\n",
      "Reward at 51000 is 1.4292549019607843\n",
      "Reward at 52000 is 1.4291538461538462\n",
      "Reward at 53000 is 1.4299622641509433\n",
      "Reward at 54000 is 1.4293148148148147\n",
      "Reward at 55000 is 1.4303454545454546\n",
      "Reward at 56000 is 1.4303928571428572\n",
      "Reward at 57000 is 1.4306666666666668\n",
      "Reward at 58000 is 1.430551724137931\n",
      "Reward at 59000 is 1.4309322033898304\n",
      "Reward at 60000 is 1.4306333333333334\n",
      "Reward at 61000 is 1.4309672131147542\n",
      "Reward at 62000 is 1.4309516129032258\n",
      "Reward at 63000 is 1.4306031746031747\n",
      "Reward at 64000 is 1.430046875\n",
      "Reward at 65000 is 1.4302307692307692\n",
      "Reward at 66000 is 1.431469696969697\n",
      "Reward at 67000 is 1.43155223880597\n",
      "Reward at 68000 is 1.43225\n",
      "Reward at 69000 is 1.4319420289855072\n",
      "Reward at 70000 is 1.4319142857142857\n",
      "Reward at 71000 is 1.4322957746478873\n",
      "Reward at 72000 is 1.432611111111111\n",
      "Reward at 73000 is 1.4325342465753426\n",
      "Reward at 74000 is 1.432310810810811\n",
      "Reward at 75000 is 1.43204\n",
      "Reward at 76000 is 1.433\n",
      "Reward at 77000 is 1.4325974025974026\n",
      "Reward at 78000 is 1.4323846153846154\n",
      "Reward at 79000 is 1.433139240506329\n",
      "Reward at 80000 is 1.43355\n",
      "Reward at 81000 is 1.4334567901234567\n",
      "Reward at 82000 is 1.4336951219512195\n",
      "Reward at 83000 is 1.4332048192771085\n",
      "Reward at 84000 is 1.433297619047619\n",
      "Reward at 85000 is 1.4337176470588235\n",
      "Reward at 86000 is 1.4336279069767441\n",
      "Reward at 87000 is 1.4335632183908047\n",
      "Reward at 88000 is 1.4339204545454545\n",
      "Reward at 89000 is 1.434191011235955\n",
      "Reward at 90000 is 1.4343666666666666\n",
      "Reward at 91000 is 1.4346263736263736\n",
      "Reward at 92000 is 1.4348913043478262\n",
      "Reward at 93000 is 1.4350860215053765\n",
      "Reward at 94000 is 1.4350531914893616\n",
      "Reward at 95000 is 1.4354315789473684\n",
      "Reward at 96000 is 1.435375\n",
      "Reward at 97000 is 1.4350515463917526\n",
      "Reward at 98000 is 1.4347959183673469\n",
      "Reward at 99000 is 1.4348686868686868\n",
      "Reward at 100000 is 1.43495\n",
      "Reward at 101000 is 1.4347722772277227\n",
      "Reward at 102000 is 1.4351666666666667\n",
      "Reward at 103000 is 1.4354271844660194\n",
      "Reward at 104000 is 1.4363173076923077\n",
      "Reward at 105000 is 1.4361714285714287\n",
      "Reward at 106000 is 1.4361792452830189\n",
      "Reward at 107000 is 1.436056074766355\n",
      "Reward at 108000 is 1.4361388888888889\n",
      "Reward at 109000 is 1.4358165137614678\n",
      "Reward at 110000 is 1.4358454545454546\n",
      "Reward at 111000 is 1.4361351351351352\n",
      "Reward at 112000 is 1.4360178571428572\n",
      "Reward at 113000 is 1.4357345132743362\n",
      "Reward at 114000 is 1.4357631578947367\n",
      "Reward at 115000 is 1.4358869565217391\n",
      "Reward at 116000 is 1.4357241379310344\n",
      "Reward at 117000 is 1.4355897435897436\n",
      "Reward at 118000 is 1.4352627118644068\n",
      "Reward at 119000 is 1.4355882352941176\n",
      "Reward at 120000 is 1.4356916666666666\n",
      "Reward at 121000 is 1.4354462809917354\n",
      "Reward at 122000 is 1.4352131147540983\n",
      "Reward at 123000 is 1.4353414634146342\n",
      "Reward at 124000 is 1.4352661290322581\n",
      "Reward at 125000 is 1.435128\n",
      "Reward at 126000 is 1.4351904761904761\n",
      "Reward at 127000 is 1.4355590551181103\n",
      "Reward at 128000 is 1.436375\n",
      "Reward at 129000 is 1.4364341085271317\n",
      "Reward at 130000 is 1.4366846153846153\n",
      "Reward at 131000 is 1.4365190839694657\n",
      "Reward at 132000 is 1.436598484848485\n",
      "Reward at 133000 is 1.4365263157894737\n",
      "Reward at 134000 is 1.4365597014925373\n",
      "Reward at 135000 is 1.4369185185185185\n",
      "Reward at 136000 is 1.4370735294117647\n",
      "Reward at 137000 is 1.4371824817518248\n",
      "Reward at 138000 is 1.4372391304347827\n",
      "Reward at 139000 is 1.4370503597122302\n",
      "Reward at 140000 is 1.4372357142857144\n",
      "Reward at 141000 is 1.4373120567375886\n",
      "Reward at 142000 is 1.437330985915493\n",
      "Reward at 143000 is 1.4373776223776225\n",
      "Reward at 144000 is 1.43725\n",
      "Reward at 145000 is 1.437303448275862\n",
      "Reward at 146000 is 1.437417808219178\n",
      "Reward at 147000 is 1.4376598639455782\n",
      "Reward at 148000 is 1.43775\n",
      "Reward at 149000 is 1.4375704697986578\n",
      "Reward at 150000 is 1.4377133333333334\n",
      "Reward at 151000 is 1.4373973509933775\n",
      "Reward at 152000 is 1.4373552631578947\n",
      "Reward at 153000 is 1.4372418300653595\n",
      "Reward at 154000 is 1.4371818181818181\n",
      "Reward at 155000 is 1.4372774193548388\n",
      "Reward at 156000 is 1.4372115384615385\n",
      "Reward at 157000 is 1.4371847133757962\n",
      "Reward at 158000 is 1.4372025316455697\n",
      "Reward at 159000 is 1.4372641509433963\n",
      "Reward at 160000 is 1.43725\n",
      "Reward at 161000 is 1.4371925465838509\n",
      "Reward at 162000 is 1.4371851851851851\n",
      "Reward at 163000 is 1.437398773006135\n",
      "Reward at 164000 is 1.4374817073170731\n",
      "Reward at 165000 is 1.4377818181818183\n",
      "Reward at 166000 is 1.4378373493975904\n",
      "Reward at 167000 is 1.437874251497006\n",
      "Reward at 168000 is 1.4377559523809524\n",
      "Reward at 169000 is 1.4377869822485208\n",
      "Reward at 170000 is 1.4380529411764706\n",
      "Reward at 171000 is 1.438157894736842\n",
      "Reward at 172000 is 1.437953488372093\n",
      "Reward at 173000 is 1.4380231213872832\n",
      "Reward at 174000 is 1.437816091954023\n",
      "Reward at 175000 is 1.4375371428571428\n",
      "Reward at 176000 is 1.4373011363636363\n",
      "Reward at 177000 is 1.4373898305084747\n",
      "Reward at 178000 is 1.4372078651685394\n",
      "Reward at 179000 is 1.4372290502793297\n",
      "Reward at 180000 is 1.4370444444444443\n",
      "Reward at 181000 is 1.4367955801104972\n",
      "Reward at 182000 is 1.436565934065934\n",
      "Reward at 183000 is 1.4364754098360655\n",
      "Reward at 184000 is 1.4364945652173913\n",
      "Reward at 185000 is 1.4363297297297297\n",
      "Reward at 186000 is 1.4364516129032259\n",
      "Reward at 187000 is 1.4362994652406418\n",
      "Reward at 188000 is 1.4362872340425532\n",
      "Reward at 189000 is 1.4364444444444444\n",
      "Reward at 190000 is 1.4363684210526315\n",
      "Reward at 191000 is 1.4364764397905758\n",
      "Reward at 192000 is 1.4364635416666667\n",
      "Reward at 193000 is 1.4362849740932642\n",
      "Reward at 194000 is 1.4361752577319589\n",
      "Reward at 195000 is 1.436148717948718\n",
      "Reward at 196000 is 1.4361683673469388\n",
      "Reward at 197000 is 1.4362588832487309\n",
      "Reward at 198000 is 1.4363535353535353\n",
      "Reward at 199000 is 1.436391959798995\n",
      "Reward at 200000 is 1.436435\n",
      "Reward at 201000 is 1.4362487562189055\n",
      "Reward at 202000 is 1.4362673267326733\n",
      "Reward at 203000 is 1.4362019704433497\n",
      "Reward at 204000 is 1.4360539215686274\n",
      "Reward at 205000 is 1.4359219512195123\n",
      "Reward at 206000 is 1.4360631067961165\n",
      "Reward at 207000 is 1.4360772946859903\n",
      "Reward at 208000 is 1.4363028846153847\n",
      "Reward at 209000 is 1.4362200956937798\n",
      "Reward at 210000 is 1.4360809523809523\n",
      "Reward at 211000 is 1.4360284360189572\n",
      "Reward at 212000 is 1.4358962264150943\n",
      "Reward at 213000 is 1.435474178403756\n",
      "Reward at 214000 is 1.4355280373831776\n",
      "Reward at 215000 is 1.4355906976744186\n",
      "Reward at 216000 is 1.4354537037037036\n",
      "Reward at 217000 is 1.4355760368663595\n",
      "Reward at 218000 is 1.435876146788991\n",
      "Reward at 219000 is 1.4359543378995434\n",
      "Reward at 220000 is 1.4359318181818181\n",
      "Reward at 221000 is 1.4356832579185521\n",
      "Reward at 222000 is 1.4356486486486486\n",
      "Reward at 223000 is 1.4356367713004485\n",
      "Reward at 224000 is 1.4355491071428572\n",
      "Reward at 225000 is 1.4357733333333333\n",
      "Reward at 226000 is 1.4356504424778762\n",
      "Reward at 227000 is 1.435590308370044\n",
      "Reward at 228000 is 1.4356271929824562\n",
      "Reward at 229000 is 1.435650655021834\n",
      "Reward at 230000 is 1.4354478260869565\n",
      "Reward at 231000 is 1.4353419913419914\n",
      "Reward at 232000 is 1.4354655172413793\n",
      "Reward at 233000 is 1.4355021459227468\n",
      "Reward at 234000 is 1.4352692307692307\n",
      "Reward at 235000 is 1.4353531914893618\n",
      "Reward at 236000 is 1.4352372881355933\n",
      "Reward at 237000 is 1.4352362869198312\n",
      "Reward at 238000 is 1.4354621848739495\n",
      "Reward at 239000 is 1.4357489539748953\n",
      "Reward at 240000 is 1.4357458333333333\n",
      "Reward at 241000 is 1.4356307053941908\n",
      "Reward at 242000 is 1.4354669421487602\n",
      "Reward at 243000 is 1.435588477366255\n",
      "Reward at 244000 is 1.435344262295082\n",
      "Reward at 245000 is 1.4353755102040817\n",
      "Reward at 246000 is 1.435390243902439\n",
      "Reward at 247000 is 1.435242914979757\n",
      "Reward at 248000 is 1.435201612903226\n",
      "Reward at 249000 is 1.4354618473895582\n",
      "Reward at 250000 is 1.435548\n",
      "Reward at 251000 is 1.4355418326693228\n",
      "Reward at 252000 is 1.4354325396825396\n",
      "Reward at 253000 is 1.4355098814229248\n",
      "Reward at 254000 is 1.4355511811023622\n",
      "Reward at 255000 is 1.4354470588235295\n",
      "Reward at 256000 is 1.4354375\n",
      "Reward at 257000 is 1.4353463035019456\n",
      "Reward at 258000 is 1.435360465116279\n",
      "Reward at 259000 is 1.4352972972972973\n",
      "Reward at 260000 is 1.435326923076923\n",
      "Reward at 261000 is 1.43544061302682\n",
      "Reward at 262000 is 1.435351145038168\n",
      "Reward at 263000 is 1.4354182509505704\n",
      "Reward at 264000 is 1.4355075757575757\n",
      "Reward at 265000 is 1.435645283018868\n",
      "Reward at 266000 is 1.4357293233082706\n",
      "Reward at 267000 is 1.435823970037453\n",
      "Reward at 268000 is 1.4357985074626867\n",
      "Reward at 269000 is 1.4356951672862452\n",
      "Reward at 270000 is 1.4356185185185186\n",
      "Reward at 271000 is 1.4356568265682657\n",
      "Reward at 272000 is 1.4356948529411764\n",
      "Reward at 273000 is 1.4356593406593408\n",
      "Reward at 274000 is 1.4357080291970803\n",
      "Reward at 275000 is 1.4355927272727274\n",
      "Reward at 276000 is 1.4356195652173913\n",
      "Reward at 277000 is 1.4355415162454874\n",
      "Reward at 278000 is 1.4357374100719424\n",
      "Reward at 279000 is 1.4357598566308243\n",
      "Reward at 280000 is 1.4357142857142857\n",
      "Reward at 281000 is 1.4357864768683275\n",
      "Reward at 282000 is 1.4358900709219857\n",
      "Reward at 283000 is 1.436\n",
      "Reward at 284000 is 1.4359788732394365\n",
      "Reward at 285000 is 1.4359964912280703\n",
      "Reward at 286000 is 1.436020979020979\n",
      "Reward at 287000 is 1.4359965156794425\n",
      "Reward at 288000 is 1.435861111111111\n",
      "Reward at 289000 is 1.435961937716263\n",
      "Reward at 290000 is 1.436048275862069\n",
      "Reward at 291000 is 1.4361099656357388\n",
      "Reward at 292000 is 1.4360924657534246\n",
      "Reward at 293000 is 1.4361467576791809\n",
      "Reward at 294000 is 1.4360748299319728\n",
      "Reward at 295000 is 1.4359084745762711\n",
      "Reward at 296000 is 1.435804054054054\n",
      "Reward at 297000 is 1.4356599326599326\n",
      "Reward at 298000 is 1.4358120805369128\n",
      "Reward at 299000 is 1.435876254180602\n",
      "Reward at 300000 is 1.4359166666666667\n",
      "Reward at 301000 is 1.4359169435215946\n",
      "Reward at 302000 is 1.4359006622516557\n",
      "Reward at 303000 is 1.4359372937293728\n",
      "Reward at 304000 is 1.435891447368421\n",
      "Reward at 305000 is 1.4359147540983606\n",
      "Reward at 306000 is 1.4359869281045752\n",
      "Reward at 307000 is 1.4358664495114006\n",
      "Reward at 308000 is 1.4357824675324675\n",
      "Reward at 309000 is 1.4357637540453074\n",
      "Reward at 310000 is 1.4356903225806452\n",
      "Reward at 311000 is 1.4358745980707395\n",
      "Reward at 312000 is 1.4359326923076923\n",
      "Reward at 313000 is 1.4358785942492012\n",
      "Reward at 314000 is 1.4358662420382167\n",
      "Reward at 315000 is 1.4359174603174603\n",
      "Reward at 316000 is 1.4359810126582278\n",
      "Reward at 317000 is 1.4360094637223975\n",
      "Reward at 318000 is 1.436003144654088\n",
      "Reward at 319000 is 1.4358369905956112\n",
      "Reward at 320000 is 1.435884375\n",
      "Reward at 321000 is 1.4358193146417446\n",
      "Reward at 322000 is 1.4358229813664596\n",
      "Reward at 323000 is 1.435640866873065\n",
      "Reward at 324000 is 1.4356882716049382\n",
      "Reward at 325000 is 1.4355569230769232\n",
      "Reward at 326000 is 1.4355398773006134\n",
      "Reward at 327000 is 1.4356299694189603\n",
      "Reward at 328000 is 1.4357225609756097\n",
      "Reward at 329000 is 1.435726443768997\n",
      "Reward at 330000 is 1.4357\n",
      "Reward at 331000 is 1.4357099697885196\n",
      "Reward at 332000 is 1.4357259036144578\n",
      "Reward at 333000 is 1.4356846846846847\n",
      "Reward at 334000 is 1.4357335329341316\n",
      "Reward at 335000 is 1.435734328358209\n",
      "Reward at 336000 is 1.4357232142857144\n",
      "Reward at 337000 is 1.4356824925816023\n",
      "Reward at 338000 is 1.435715976331361\n",
      "Reward at 339000 is 1.4357345132743362\n",
      "Reward at 340000 is 1.4357558823529413\n",
      "Reward at 341000 is 1.435841642228739\n",
      "Reward at 342000 is 1.4358538011695907\n",
      "Reward at 343000 is 1.435804664723032\n",
      "Reward at 344000 is 1.4357383720930232\n",
      "Reward at 345000 is 1.435750724637681\n",
      "Reward at 346000 is 1.4357976878612717\n",
      "Reward at 347000 is 1.4357463976945244\n",
      "Reward at 348000 is 1.435528735632184\n",
      "Reward at 349000 is 1.4353638968481375\n",
      "Reward at 350000 is 1.43548\n",
      "Reward at 351000 is 1.4353618233618233\n",
      "Reward at 352000 is 1.4353380681818182\n",
      "Reward at 353000 is 1.4353087818696884\n",
      "Reward at 354000 is 1.435189265536723\n",
      "Reward at 355000 is 1.435143661971831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimulation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddqn_simulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDQNSimulate\n\u001b[1;32m      3\u001b[0m simulate \u001b[38;5;241m=\u001b[39m DDQNSimulate()\n\u001b[0;32m----> 4\u001b[0m \u001b[43msimulate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/personal/graduation_thesis/drl_sample/simulation/ddqn_simulation.py:21\u001b[0m, in \u001b[0;36mDDQNSimulate.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_state_deep()\n\u001b[1;32m     20\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, (\u001b[38;5;241m1\u001b[39m, num_features))\n\u001b[0;32m---> 21\u001b[0m list_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m list_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_possible_action()\n\u001b[1;32m     24\u001b[0m max_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/engine/training.py:2033\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   2032\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2033\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2034\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   2035\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:534\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    525\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m functional_ops\u001b[38;5;241m.\u001b[39mpartitioned_call(\n\u001b[1;32m    526\u001b[0m             args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    527\u001b[0m             f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m             config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    531\u001b[0m             executor_type\u001b[38;5;241m=\u001b[39mexecutor_type)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, func_graph_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_graph_outputs):\n\u001b[0;32m--> 534\u001b[0m   \u001b[43mhandle_data_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_graph_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executing_eagerly:\n\u001b[1;32m    536\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/handle_data_util.py:40\u001b[0m, in \u001b[0;36mcopy_handle_data\u001b[0;34m(source_t, target_t)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy_handle_data\u001b[39m(source_t, target_t):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Copies HandleData for variant and resource type tensors if available.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m  The CppShapeInferenceResult::HandleData proto contains information about the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    target_t: The tensor to copy HandleData to.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mtarget_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     41\u001b[0m       target_t\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mvariant):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source_t, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[1;32m     43\u001b[0m       handle_data \u001b[38;5;241m=\u001b[39m source_t\u001b[38;5;241m.\u001b[39m_handle_data  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:186\u001b[0m, in \u001b[0;36mDType.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_type_enum\u001b[49m \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39m_type_enum\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from simulation.ddqn_simulation import DDQNSimulate\n",
    "\n",
    "simulate = DDQNSimulate()\n",
    "simulate.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
